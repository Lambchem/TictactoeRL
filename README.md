# Tic-Tac-Toe Reinforcement Learning

本项目实现了一个基于 **强化学习 (Q-Learning)** 的井字棋 AI。通过数万轮的自我对弈（Self-play），该 AI 已完全掌握井字棋的最优博弈策略，达到了“上帝视角”：在完美对弈下永不落败，并能精准捕捉对手的任何微小失误。

## 🌟 核心亮点

- **完全收敛**：AI 已学习到井字棋的必和本质（Nash Equilibrium），在先手或后手均能维持完美对弈。
- **对称性优化**：利用棋盘的 8 种对称性（旋转与翻转），将状态空间从数千个压缩至几百个，极大地提高了训练效率和逻辑一致性。
- **博弈论分析**：内置工具可导出“开局书”（Opening Book），包含每个局面的 Q 值评估与胜率预测。
- **多种模式**：支持人机对弈、AI 自我对弈演示以及模型训练。

---

## 📂 文件结构

| 文件 | 功能描述 |
| :--- | :--- |
| `Train.py` | **训练核心**：AI 通过自我对弈学习 Q-Table，支持 Minimax-Q 更新逻辑。 |
| `book.py` | **分析工具**：处理棋盘对称性，将二进制模型导出为人类可读的 `opening_book.txt`。 |
| `humanvsai.py`| **人机交互**：在这里挑战已经训练完成的 AI。 |
| `aivsai.py` | **演示模式**：观察两个完美的 AI 如何进行巅峰对决。 |
| `q.pkl` | **模型文件**：存储训练好的 Q-Table 权重（由训练生成）。 |

---

## 🚀 快速开始

### 1. 环境准备
项目仅依赖 Python 3 标准库，无需额外安装复杂框架：
```bash
# 克隆仓库
git clone https://github.com/YourUsername/TicTacToe-RL.git
cd TicTacToe-RL
```

### 2. 训练 AI
如果你想从头训练或继续训练模型：
```bash
python Train.py
```
默认设置下，AI 会进行 1,000,000 轮训练并定期输出评估结果。

### 3. 生成并查看“开局书”
运行 `book.py` 分析当前 AI 的逻辑深度：
```bash
python book.py
```
它会生成 `opening_book.txt`。你会发现当先手占角时，AI 作为后手会报出唯一的生路（中心位 Q=0），而其他位置均为必败（Q < 0）。

### 4. 挑战 AI
准备好接受失败了吗？
```bash
python humanvsai.py
```

---

## 🧠 技术原理

### Q-Learning 逻辑
AI 使用一个字典存储状态-动作对的价值（Q-Value）。
更新公式结合了 **Minimax** 思想：
$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma (\text{Opponent's Worst Case}) - Q(s, a)]$$
这意味着 AI 在决策时，不仅考虑自己的收益，还预判对手会做出对其最不利的完美回应。

### 对称归一化 (Symmetry Normalization)
井字棋具有高度的对称性。本项目通过对棋盘进行旋转和翻转，将 8 个本质相同的物理状态映射为同一个逻辑状态。
- **好处**：学习速度提升 8 倍，且保证了 AI 在面对棋盘不同方位时的表现绝对一致。

### 胜率映射
Q 值线性映射至 $[0, 1]$ 区间，用于预测胜率：
- `0.5`：理论平局
- `1.0`：己方必胜
- `0.0`：己方必败

---

## 📈 实验观察
经过训练后，AI 表现出以下特征：
1. **根节点平衡**：空棋盘时所有位置 Q=0，认可井字棋的必和属性。
2. **强迫封堵**：当对手即将连成一线时，对应封堵位的 Q 值显著高于其他位置。
3. **诱敌陷阱**：在平局分支中，AI 倾向于选择那些让对手最容易犯错的路径。

---

## 📄 许可证
[MIT License](LICENSE)
